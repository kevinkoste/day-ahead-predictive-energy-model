{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "import numpy as np\n",
    "\n",
    "import utility_functions as fn\n",
    "\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manually define building names\n",
    "buildingnames = ['YUAG',\n",
    "                 'Berkeley',\n",
    "                 'Hopper',\n",
    "                 '304Elm',\n",
    "                 'Davenport',\n",
    "                 '38HH',\n",
    "                 '320Temple',\n",
    "                 '53Wall',\n",
    "                 'Sprague',\n",
    "                 'Malone',\n",
    "                 'Trumbull',\n",
    "                 '17HH']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consumption data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read from csv into a new dataframe\n",
    "raw = pd.read_csv('data/energy_raw.csv',index_col=0,na_values=['#########'])\n",
    "\n",
    "# reindex by appropriate datetime\n",
    "raw.index = pd.to_datetime(raw.index,format='%a %m/%d/%y %H:00')\n",
    "\n",
    "# add missing rows by full reindexing\n",
    "correct_dt = pd.DatetimeIndex(start='2018-01-01 00:00:00',end='2018-07-27 23:00:00',freq='h')\n",
    "raw = raw.reindex(index=correct_dt)\n",
    "\n",
    "# remove built-in demand values, which tend to be bugged\n",
    "raw = raw.drop(raw.columns[np.arange(0,len(buildingnames)*2,2)], axis=1)\n",
    "\n",
    "# rename columns accordingly\n",
    "raw.columns = buildingnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove impossible outliers indicated by negative percent change\n",
    "raw_head = raw.iloc[0]\n",
    "raw = raw.where(raw.pct_change(limit=1)>0)\n",
    "raw.iloc[0] = raw_head\n",
    "\n",
    "# method to find outliers, not currently implemented\n",
    "# tau = 0.0005 * (raw.max()-raw.min())\n",
    "# raw = raw.where(raw.pct_change(limit=1)<tau)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate gaps in consumption data 6 hours and shorter\n",
    "for k in raw.columns:\n",
    "    raw[k] = fn.limited_impute(raw[k],6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demand data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# creates a new dataframe for the demand values, drops the first row\n",
    "demand = raw.diff().drop(raw.index[0])\n",
    "\n",
    "# saves head to replace later\n",
    "demand_head = demand.iloc[0:4]\n",
    "\n",
    "errors = demand.isnull().sum()\n",
    "print(errors)\n",
    "\n",
    "fn.plot_all(demand,'2018-01-01 01:00:00','2018-07-27 23:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove huge statistical outliers\n",
    "demand = demand.where(demand > demand.median() - 2.5*demand.std())\n",
    "demand = demand.where(demand < demand.median() + 5*demand.std())\n",
    "\n",
    "new_errors = demand.isnull().sum() - errors\n",
    "print(new_errors)\n",
    "errors = demand.isnull().sum()\n",
    "\n",
    "fn.plot_all(demand,'2018-01-01 01:00:00','2018-07-27 23:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterative (SLOW) method to remove single spikes, unsure how helpful it is\n",
    "\n",
    "# for k in demand.columns:\n",
    "#     for i in range(len(demand)-1):\n",
    "#         if (abs(demand.pct_change()[k][i]) > 0.5) & (abs(demand.pct_change()[k][i+1]) > 0.5):\n",
    "#             demand[k][i] = np.NaN\n",
    "\n",
    "            \n",
    "new_errors = demand.isnull().sum() - errors\n",
    "print(new_errors)\n",
    "errors = demand.isnull().sum()\n",
    "\n",
    "fn.plot_all(demand,'2018-01-01 01:00:00','2018-07-27 23:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove errors by rolling min and max within 10-day chunks\n",
    "chunk_size = 360\n",
    "i=0\n",
    "\n",
    "while i < len(demand):\n",
    "    end = i+chunk_size\n",
    "    if end > len(demand): end = len(demand)\n",
    "    demand[i:end].where(demand[i:end] > demand[i:end].rolling(18).min().median()*0.7, inplace=True)\n",
    "    demand[i:end].where(demand[i:end] < demand[i:end].rolling(18).max().median()*1.3, inplace=True)\n",
    "    i = i+chunk_size\n",
    "\n",
    "\n",
    "new_errors = demand.isnull().sum() - errors\n",
    "print(new_errors)\n",
    "errors = demand.isnull().sum()\n",
    "\n",
    "fn.plot_all(demand,'2018-01-01 01:00:00','2018-07-27 23:00:00')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_energy = demand.copy(deep=True)\n",
    "\n",
    "# interpolate gaps shorter than 6 hours\n",
    "for k in dense_energy.columns:\n",
    "    dense_energy[k] = fn.limited_impute(dense_energy[k],6)\n",
    "    \n",
    "fn.plot_all(dense_energy,'2018-03-05 01:00:00','2018-03-10 23:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# interpolate gaps longer than 6 hours, linear for now, to be replaced by squared-sinusoidal\n",
    "dense_energy.interpolate(method='linear',inplace=True)\n",
    "\n",
    "fn.plot_all(dense_energy,'2018-03-05 01:00:00','2018-03-10 23:00:00')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace head for final export\n",
    "dense_energy.iloc[0:4] = demand_head\n",
    "\n",
    "# export clean data to csv\n",
    "dense_energy.round(1).to_csv('data/energy_clean.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### unused code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # this cell removes all consumption values BELOW first value\n",
    "\n",
    "# # creates dataframe of repeated minimum (first) values, pretty workaroundy\n",
    "# raw_mins = raw.copy(deep=True)\n",
    "# raw_mins.loc[:,:] = raw.loc['2018-01-01 00:00:00'].values\n",
    "\n",
    "# # sets all violating values to NaN\n",
    "# raw = raw.where(raw >= raw_mins)\n",
    "\n",
    "# # this unused line was an attempt to find outliers using std ranges\n",
    "# # raw = raw.where(raw > raw.median() - 2*raw.std()).where(raw < raw.median() + 2*raw.std())\n",
    "\n",
    "# raw.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to plot daily average curves for buildings... matplotlib might support this somehow or i can write a fn\n",
    "\n",
    "# raw = raw.iloc[:,[0]]\n",
    "\n",
    "# raw['day'] = raw.index.day\n",
    "# raw['hour'] = raw.index.hour\n",
    "\n",
    "# raw_by_day = raw.resample('h').mean()\n",
    "# raw_by_day = raw_by_day.set_index(['day','hour']).unstack('day')\n",
    "# raw_by_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OLD ATTEMPT to find outliers using tuner values\n",
    "\n",
    "# comp = demand.take([1], axis=1)\n",
    "# comp['rolling'] = comp.loc[:,'Berkeley'].rolling(4,min_periods=1).median()\n",
    "# # demand.rolling(4,min_periods=1).median()*0.5\n",
    "\n",
    "# comp['rollmin'] = comp.loc[:,'Berkeley'].rolling(8,min_periods=6).min()\n",
    "\n",
    "# # fn.plot_all(comp,'2018-03-14 00:00:00','2018-06-20 00:00:00')\n",
    "\n",
    "# fn.plot_feature(comp,'rollmin','2018-01-02 00:00:00','2018-07-25 00:00:00')\n",
    "\n",
    "# comp['rollmin'].median()-comp['rollmin'].std()\n",
    "\n",
    "# max_tuner = 1 # here i want higher values clipping more points\n",
    "# min_tuner = 1\n",
    "\n",
    "# # demand.std()/demand.median() # here high values indicate volatile, \n",
    "\n",
    "# demand = demand.where(demand > demand.rolling(8,min_periods=4).min() - (min_tuner - demand.std()/demand.median()))\n",
    "                      \n",
    "# demand = demand.where(demand < demand.rolling(8,min_periods=4).max() + (max_tuner - demand.std()/demand.median()))\n",
    "\n",
    "# fn.plot_all(demand,'2018-01-02 00:00:00','2018-07-25 00:00:00')\n",
    "# demand.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterative (bad performance) solution for single spikes, could work as supplement\n",
    "\n",
    "# k='YUAG'\n",
    "\n",
    "# for i in range(len(demand)-1):\n",
    "#     if (abs(demand.pct_change()[k][i]) > 0.5) & (abs(demand.pct_change()[k][i+1]) > 0.5):\n",
    "#         demand[k][i] = np.NaN\n",
    "\n",
    "# fn.plot_all(demand,'2018-01-02 00:00:00','2018-07-25 00:00:00')\n",
    "# demand.isnull().sum()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
