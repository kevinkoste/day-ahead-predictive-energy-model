{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "import numpy as np\n",
    "\n",
    "# plotting libraries\n",
    "import seaborn as sns\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# models\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell constructs a Series called y that contains the target feature - in this case, hourly electricity demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read clean electricity data to create target variable array y\n",
    "elec = pd.read_csv('../data/elec_clean.csv',index_col=0)\n",
    "elec.index = pd.to_datetime(elec.index,format='%Y-%m-%d %H:00:00')\n",
    "\n",
    "# remove Davenport for now because it is missing most of June and July\n",
    "elec.drop('Davenport',axis=1,inplace=True)\n",
    "\n",
    "y = pd.Series(elec.iloc[:, 0:11].sum(axis=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell constructs a DataFrame called X that contains the covariate data. Here, one-hot dates, weather lag, and electricity lag features are added for a total of 45 covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create X as a copy of y\n",
    "X = pd.DataFrame(index=y.index)\n",
    "\n",
    "# add electricity lag features (be careful with this)\n",
    "elec = pd.DataFrame(index=y.index)\n",
    "for i in range(1,7):\n",
    "    elec['elec -'+str(i)] = np.append(np.array([np.nan]*i),y.values[0:len(y)-i])\n",
    "X = X.join(elec)\n",
    "\n",
    "# add weather lag features\n",
    "weather = pd.read_csv('../data/weather_clean.csv',index_col=0)\n",
    "weather.index = pd.to_datetime(weather.index,format='%Y-%m-%d %H:00:00')\n",
    "weather = weather.loc[y.index[0]:y.index[len(y)-1]]\n",
    "X = X.join(weather)\n",
    "\n",
    "# add boolean date features\n",
    "dates = pd.read_csv('../data/one_hot_dates.csv',index_col=0)\n",
    "dates.index = pd.to_datetime(dates.index,format='%Y-%m-%d %H:00:00')\n",
    "X = X.join(dates)\n",
    "\n",
    "# truncate y and X to match each other (this step is only necessary if using elec lag features)\n",
    "X = X.iloc[6:]\n",
    "y = y.iloc[6:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This cell defines the train and test sets for a random day in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose a random day between Jan 10 and July 10 to test\n",
    "split_times = pd.date_range(start='2018-01-10 08:00:00',end='2018-07-10 08:00:00',freq='D')\n",
    "split_time = split_times[np.random.randint(len(split_times))]\n",
    "end_time = split_time + pd.DateOffset(hours=39)\n",
    "\n",
    "# define train set before the split and test the following day\n",
    "X_train = X.loc[X.index <= split_time].copy()\n",
    "X_test = X.loc[(X.index > split_time) & (X.index <= end_time)].copy()\n",
    "y_train = y.loc[y.index <= split_time].copy()\n",
    "y_test = y.loc[(y.index > split_time) & (y.index <= end_time)].copy()\n",
    "\n",
    "# fit standard scalers to the weather and elec features in the training data\n",
    "weather_fit = StandardScaler().fit(X_train[weather.columns])\n",
    "elec_fit = StandardScaler().fit(X_train[elec.columns])\n",
    "\n",
    "# scale the training and test sets using the scalers fit to the training set\n",
    "X_train[weather.columns] = weather_fit.transform(X_train[weather.columns])\n",
    "X_train[elec.columns] = elec_fit.fit_transform(X_train[elec.columns])\n",
    "X_test[weather.columns] = weather_fit.transform(X_test[weather.columns])\n",
    "X_test[elec.columns] = elec_fit.transform(X_test[elec.columns])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here I would like to implement an iterative linear regression model to more accurately represent the reality of placing bids in the day ahead market. These models will have a test set of just one hour, and the result of each model will be used to predict the following hour.\n",
    "\n",
    "This is not working right now because the scaler can't be applied to a single row...\n",
    "Should seek advice on this from statlab or other expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected 2D array, got 1D array instead:\narray=[282.22 100.   281.89 281.65 281.75 281.69 281.33 281.24  87.   100.\n 100.   100.   100.    93.  ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-b5a146fa253b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweather\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweather_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweather\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melec_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweather\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweather_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mweather\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m     \u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0melec_fit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0melec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/preprocessing/data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X, y, copy)\u001b[0m\n\u001b[1;32m    750\u001b[0m         X = check_array(X, accept_sparse='csr', copy=copy, warn_on_dtype=True,\n\u001b[1;32m    751\u001b[0m                         \u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mFLOAT_DTYPES\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 752\u001b[0;31m                         force_all_finite='allow-nan')\n\u001b[0m\u001b[1;32m    753\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    550\u001b[0m                     \u001b[0;34m\"Reshape your data either using array.reshape(-1, 1) if \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                     \u001b[0;34m\"your data has a single feature or array.reshape(1, -1) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \"if it contains a single sample.\".format(array))\n\u001b[0m\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;31m# in the future np.flexible dtypes will be handled like object dtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Expected 2D array, got 1D array instead:\narray=[282.22 100.   281.89 281.65 281.75 281.69 281.33 281.24  87.   100.\n 100.   100.   100.    93.  ].\nReshape your data either using array.reshape(-1, 1) if your data has a single feature or array.reshape(1, -1) if it contains a single sample."
     ]
    }
   ],
   "source": [
    "# choose a random day between Jan 10 and July 10 to test\n",
    "split_times = pd.date_range(start='2018-01-10 08:00:00',end='2018-07-10 08:00:00',freq='D')\n",
    "split_time = split_times[np.random.randint(len(split_times))]\n",
    "target_hour = split_time + pd.DateOffset(hours=1)\n",
    "\n",
    "\n",
    "for i in range(1,5):\n",
    "    target_hour = split_time + pd.DateOffset(hours=i)\n",
    "    \n",
    "    X_train = X.loc[X.index < target_hour].copy()\n",
    "    X_test = X.loc[target_hour].copy()\n",
    "    y_train = y.loc[y.index < target_hour].copy()\n",
    "    y_test = y.loc[target_hour].copy()\n",
    "\n",
    "    # fit standard scalers to the weather and elec features in the training data\n",
    "    weather_fit = StandardScaler().fit(X_train[weather.columns])\n",
    "    elec_fit = StandardScaler().fit(X_train[elec.columns])\n",
    "\n",
    "    # scale the training and test sets using the scalers fit to the training set\n",
    "    X_train[weather.columns] = weather_fit.transform(X_train[weather.columns])\n",
    "    X_train[elec.columns] = elec_fit.fit_transform(X_train[elec.columns])\n",
    "    X_test[weather.columns] = weather_fit.transform(X_test[weather.columns])\n",
    "    X_test[elec.columns] = elec_fit.transform(X_test[elec.columns])\n",
    "    \n",
    "    # train the model\n",
    "    reg = model.fit(X_train, y_train)\n",
    "    y_pred = reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.70295229507647\n"
     ]
    }
   ],
   "source": [
    "mape = np.mean(np.abs((y_test - y_pred) / y_test))*100\n",
    "print(mape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
